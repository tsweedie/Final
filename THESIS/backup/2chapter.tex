\chapter{Related Work}

There are a wide variety of methods that are used in the field of emotion recognition using facial expression, however a large number of those methods are implemented using a similar process. Initially the image is captured and processed, thereafter the face is detected and features are extracted. Then a learning technique is trained to do the classification of the emotions. The related work looks at all 4 aspects of the implementation process and different methods used by other researchers.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.6]{4}
  \caption{4 stages for facial expression recognition}
\end{figure}
%---------------------------------------------------------------
\section{Capture Image from the camera and image processing}
An image is taken from the camera and image processing tools are used to help normalize the input image.
\begin{itemize}

\item Goyal and Mittal, achieved the desired resolution and colour for their images by adjusting the brightness and contrast of the image\cite{1}.
\item Reddy and Srinivas, scaled and cropped their images to $(150 \times 120)$, and ensured that the location of the eyes was the same in each image. The image is further processed, using an average combination of all the input image histograms. This process is called histogram equalization and helps in decreasing variation in an image. Histogram equalization is a technique for stretching out the intensity range of an image to enhance the contrast of the image\cite{2}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.4]{5}
  \caption{Histogram Equalization}
\end{figure}

\item Boubenna and Lee, scaled their images to $(100 \times 100)$ pixels\cite{3}.

\end{itemize}
%---------------------------------------------------------------
\section{Face Detection - finding the face  }

Finding the location of the face helps identify the region that contains all the features required to continue with facial expression recognition. The rest of the image is not as important for this purpose.
\begin{itemize}
\item Reddy and Srinivas, applied a fixed oval shaped mask over the image to extract the face region\cite{12}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.4]{6}
  \caption{Preprocessed image with Oval Masks}
\end{figure}

\item Boubenna and Lee, used the Viola and Jones algorithm to detect the location of the face in an image. \cite{viola}This algorithm uses Haar-like features to help find the facial features, such as the eyes , nose and mouth. The Ada Boost algorithm is used to reduce the number of features, if there are too many. They used the canny edge detection operator to detect the edges of the face.\cite{3}
\end{itemize}

%---------------------------------------------------------------

\section{Feature Extraction- extract the features from the face }
Once the face has been detected, it is important to identify which features will be used for feature extraction (eyes, nose, mouth, eyebrows, full-face etc.). The feature extraction algorithm can be applied based on its compatibility with the features chosen. 
\begin{itemize}
\item\ Goyal and Mittal, extracted the nose, mouth and eyes using the Viola and Jones Haar classifier\cite{1}.
\item Reddy and Srinivas, considered the entire face for the feature extraction not just the eyes, mouth and nose individually . First, they used Gabor filters to generate a bank of filters at 5 spatially varying frequencies and 8 orientations. The filtered outputs were then concatenated and used PCA(Principle Component Analysis) to reduce dimensionality. The PCA algorithm will generate the eigenfaces for each image of dimension$(N \times N)$. From this their system generated the eigenvector of dimension \textit{(N2)} for each image. PCA is a statistical technique that reduces the dimensions of feature vectors. The high dimensionality of feature vectors can cause over-fitting during classification. The vectors that relay the distribution of the face images the best are selected, and are used to define the subspace(“face space”) of the face images\cite{2}.

\item Boubenna and Lee, used Pyramid Histogram of Oriented Gradients(PHOG) to extract features. PHOG represents an image by its local shape and the spatial layout. The local shape of an image is represented by a histogram of edge orientations within an image sub-region, which are divided into K bins. The Spatial layout is represented by tiling the image into regions at different levels. Each image is divided into a sequence of increasingly finer spatial grids by repeatedly doubling the number of divisions in each axis direction\cite{phog}. The parameters of PHOG were set as follows: 3 for number of level, 360 degree for angle and 16 for number of bins. To decrease the number of features a Genetic Algorithm(GA) is used, which resembles natural selection to find the optimal features\cite{3}.
\end{itemize}

%---------------------------------------------------------------

\section{Train machine learning technique}
The training of the machine learning technique based on supervised learning. Where the machine learning technique is given labelled images (Happy, Sad, Anger, Disgust, Surprise, Fear and Neutral) and is required to learn them. Once the machine learning technique has completed its training it can then be fed unlabelled images, and the result would be a prediction of which label best suits the given image. 
\begin{itemize}
\item Goyal and Mittal,used an Artificial Neural Network, with one hidden layer. The neural network architecture has three types layers: input, hidden and output layers. Feed-forward ANNs allows the signal to travel in one direction from the input layer to the output layer. Recurrent networks contain feedback connections, where the signal moves in both directions. To get accurate results from the ANN, the weights can be set explicitly using prior knowledge or the ANN can be trained to help find the optimal weights\cite{1, ann}.
\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.4]{8}
  \caption{Architecture of an artificial neuron and a multilayered neural network }
\end{figure}
\item Reddy and Srinivas, used an Artificial Neural Network, with two hidden layers\cite{2}. 
\item Boubenna and Lee, used Linear Discriminant Analysis(LDA) and K Nearest Neighbours(KNN). LDA finds the maximum distances within classes, to obtain maximum class separation. LDA only uses up to second order moments, such as the covariance and mean, of the class distribution. KNN classifies unlabelled samples according to the training samples. KNN finds the nearest K in the labelled samples and set them to the closest group, for the unlabelled samples. One distance measure is required, and \cite{3} used the cosine distance measure.
\end{itemize}
%---------------------------------------------------------------
\section{Results}
The results from the three studies are as follows, with \cite{3} having the best overall results for their human facial expression system.
\begin{itemize}
\item Goyal and Mittal, achieved a 80\% Right classification, using a confusion matrix and a regression plot to verify the results\cite{1}.
\item Reddy and Srinivas, achieved a 85.7\% Right classification using the JAFFE database\cite{2}.
\item Boubenna and Lee, achieved a 99.33\% accuracy, using the Radboud Faces Database(RaFD)\cite{3}.
\end{itemize}


